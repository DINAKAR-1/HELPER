ğŸ”¥ Kafka Interview Questions & Answers (2-Year Experience, Spring Boot + Java Full Stack)
1. What is Kafka and why do we use it?

ğŸ‘‰ Answer:
Kafka is a distributed event streaming platform. Instead of services calling each other directly (tight coupling), Kafka lets them talk via events/messages.
We use it for:

Decoupling services (producer doesnâ€™t care who consumes).

Handling high throughput (millions of events/sec).

Scalability & fault tolerance (replication, partitioning).

Event-driven architecture (real-time processing).

2. What are Kafka Topics, Partitions, and Offsets?

ğŸ‘‰ Answer:

Topic â†’ A category/feed name (like a table in DB).

Partition â†’ A topic is split into multiple partitions for parallelism.

Offset â†’ Each message in a partition gets a unique sequential ID.

Example: If topic orders has 3 partitions, producers can publish to all 3. Consumers track offsets to know whatâ€™s read.

3. How does Kafka ensure fault tolerance?

ğŸ‘‰ Answer:

Replication: Each partition has a leader and multiple replicas.

If a leader broker dies, a replica is promoted.

Consumers and producers reconnect automatically.

ACKs (acks=0,1,all) ensure durability at different levels.

4. How do you integrate Kafka with Spring Boot?

ğŸ‘‰ Answer:
Use Spring Kafka dependency.

Configure producer/consumer in application.yml.

Use @KafkaListener to consume.

Use KafkaTemplate to produce.

Example:

// Producer
@Autowired
private KafkaTemplate<String, String> kafkaTemplate;

public void sendMessage(String msg) {
    kafkaTemplate.send("orders", msg);
}

// Consumer
@KafkaListener(topics = "orders", groupId = "order-group")
public void consume(String message) {
    System.out.println("Consumed: " + message);
}

5. Whatâ€™s the difference between Kafka and traditional message brokers (like RabbitMQ/ActiveMQ)?

ğŸ‘‰ Answer:

Kafka â†’ Designed for high throughput, scalability, event streaming. Messages are persisted for a configurable time. Consumers can replay.

RabbitMQ â†’ Good for request-response, low latency, smaller workloads, but not built for heavy event streaming.

6. Whatâ€™s a Consumer Group in Kafka?

ğŸ‘‰ Answer:

A consumer group is a set of consumers that share the work of consuming a topic.

Kafka ensures each partition is consumed by only one consumer in the group.

Scaling consumers = scaling partitions.

Example: If topic has 3 partitions and consumer group has 3 consumers â†’ each consumer gets 1 partition.

7. How do you handle message ordering in Kafka?

ğŸ‘‰ Answer:

Kafka guarantees ordering within a partition, not across partitions.

To ensure order:

Send related events to the same partition using a key.

E.g., all messages for customerId=123 go to the same partition.

8. How do you commit offsets in Kafka?

ğŸ‘‰ Answer:

Automatic commit â†’ Kafka commits offsets periodically. Risk: message might be processed twice.

Manual commit â†’ Developer controls when to mark message as consumed. Safer if you need "at-least-once" processing.

9. What are delivery semantics in Kafka?

ğŸ‘‰ Answer:

At most once: Messages may be lost but never redelivered.

At least once (default): No message lost, but duplicates possible.

Exactly once: Guaranteed no loss, no duplicates (via idempotent producer + transactional consumer).

10. Can Kafka be used in a microservices architecture?

ğŸ‘‰ Answer:
Yes. Kafka acts as the backbone for event-driven microservices:

Services publish events (producer).

Other services consume (consumer).

Loose coupling, scalability, and resilience.

11. How do you secure Kafka?

ğŸ‘‰ Answer:

Authentication: SASL (SCRAM, Kerberos).

Authorization: ACLs.

Encryption: TLS for data in transit.

12. What are some real-world use cases youâ€™ve seen/implemented with Kafka?

ğŸ‘‰ Answer (customize for interviews):

Logging pipeline (collect logs from microservices into Elasticsearch).

Order management (producers â†’ order events, consumers â†’ update inventory, billing).

Real-time analytics (Kafka + Spark/Flink).

13. How does Kafka differ from a database?

ğŸ‘‰ Answer:

Kafka = commit log + streaming. Not for queries/joins like SQL.

Database = storage + query engine.

Kafka is often used alongside DB, not instead of it.

14. Whatâ€™s the role of Zookeeper in Kafka?

ğŸ‘‰ Answer:

(Before Kafka 2.8) Zookeeper managed brokers, partitions, leader election.

(Now) Kafka has KRaft mode (self-managed) â†’ Zookeeper is optional and being phased out.

15. What happens if a consumer is slower than producer?

ğŸ‘‰ Answer:

Kafka stores data for a configured retention period.

Slow consumer can catch up later unless retention time expires.

If retention expires â†’ data is lost for that consumer.
======================================================================================

âš¡ Level 2 & Level 3 Kafka Interview Questions (With Answers & Explanations)
1. Whatâ€™s the difference between acks=0, acks=1, and acks=all in Kafka producers?

ğŸ‘‰ Answer:

acks=0 â†’ Producer doesnâ€™t wait for broker acknowledgment. Fastest, but messages can be lost.

acks=1 â†’ Producer waits for leader only. If leader crashes before replication, data can be lost.

acks=all â†’ Producer waits until all replicas acknowledge. Slowest, but safest (no data loss).

âš¡ Why it matters: Shows you understand durability guarantees.

2. What is an idempotent producer?

ğŸ‘‰ Answer:

Normally, retries can cause duplicate messages.

Idempotent producer (enabled via enable.idempotence=true) ensures exactly-once delivery to a partition.

Uses a unique producer ID + sequence number to deduplicate.

âš¡ Why it matters: Interviewers love to test if you know how Kafka avoids duplicates.

3. How does consumer rebalancing work?

ğŸ‘‰ Answer:

In a consumer group, Kafka ensures each partition is assigned to exactly one consumer.

When a new consumer joins/leaves, partitions are reassigned â†’ this is called rebalance.

During rebalance, consumers stop processing temporarily â†’ can cause latency.

âš¡ Best practices: Use cooperative rebalancing (KIP-429) in newer Kafka for smoother transitions.

4. Difference between poll() and @KafkaListener in Spring Boot?

ğŸ‘‰ Answer:

poll() API â†’ Low-level Kafka consumer. You call poll(timeout) manually, handle offsets yourself.

@KafkaListener â†’ Spring abstraction. Handles poll loop, thread management, offset commits for you.

Use poll() when you need fine-grained control, else @KafkaListener is easier.

âš¡ Why it matters: Many devs only know @KafkaListener, but interviews want the low-level insight.

5. Whatâ€™s a Dead Letter Queue (DLQ) in Kafka?

ğŸ‘‰ Answer:

If a consumer canâ€™t process a message (due to validation error, bad JSON, etc.), you donâ€™t want to lose it.

Instead, send it to a DLQ topic.

Later, devs can debug/reprocess from DLQ.

âš¡ Why it matters: Shows you understand error handling strategies in real systems.

6. How can you tune Kafka performance?

ğŸ‘‰ Answer:

Increase partitions â†’ More parallelism.

Batching (batch.size) â†’ Send messages in chunks instead of one by one.

linger.ms â†’ Small delay to allow batching.

Compression (snappy, lz4) â†’ Reduce network overhead.

Async I/O â†’ Donâ€™t block producer threads.

âš¡ Pro tip: More partitions â‰  always better. Too many = overhead for brokers & consumers.

7. What is Schema Registry and why do we need it?

ğŸ‘‰ Answer:

Kafka itself stores raw bytes. Without a schema, consumers need to â€œguessâ€ structure.

Schema Registry (Confluent) ensures producers & consumers agree on data format (Avro, Protobuf, JSON Schema).

Supports schema evolution (backward/forward compatibility).

âš¡ Why it matters: If you ever say â€œKafka + Avro + Schema Registryâ€, the interviewer will assume youâ€™ve seen real enterprise Kafka setups.

8. Difference between Kafka Streams and a normal consumer?

ğŸ‘‰ Answer:

Normal Consumer: Just reads messages, processes, maybe writes to DB.

Kafka Streams API: Library for stream processing â†’ filtering, joins, windowing, aggregations directly on Kafka topics.

Example: orders + payments â†’ join into completed_orders.

âš¡ Why it matters: Kafka Streams = â€œreal-time ETLâ€. Good to at least know the buzzwords.

9. How does Kafka handle data retention?

ğŸ‘‰ Answer:

Messages are stored for a configurable time (retention.ms) or until log size limit (retention.bytes).

Even if consumed, messages stay until retention kicks in.

Useful for replays (consumer can reset offset).

âš¡ Common trap: Kafka is not like RabbitMQ â†’ it doesnâ€™t delete messages immediately after consumption.

10. How is Kafka monitored in production?

ğŸ‘‰ Answer:

Metrics:

Lag (how far consumers are behind).

Throughput (messages/sec).

Broker health (disk usage, partition leaders).

Tools:

Prometheus + Grafana â†’ monitoring.

Confluent Control Center â†’ enterprise UI.

âš¡ Why it matters: Knowing monitoring = youâ€™ve seen Kafka beyond tutorials.
